{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87f65049",
   "metadata": {},
   "source": [
    "## READ CHUNK CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687dcc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'category', 'subcategory', 'topic_title', 'details',\n",
      "       'last_update', 'reference_source', 'keywords', 'note'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('chunks.csv')\n",
    "keys = df.keys()\n",
    "print(keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada82e8b",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d27ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 51 knowledge chunks from 51 rows (1 row = 1 chunk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: ./RAG_database/knowledge_info.json\n",
      "âœ… Saved: ./RAG_database/knowledge_faiss_index.bin\n",
      "âœ… Saved: ./RAG_database/knowledge_tfidf_vectorizer.pkl\n",
      "âœ… Saved: ./RAG_database/knowledge_tfidf_matrix.npz\n",
      "Embedding dim: 384 | #docs: 51\n",
      "First chunk: Topic: à¸šà¸£à¸´à¸à¸²à¸£à¸‚à¸­à¸‡à¸¨à¸¹à¸™à¸¢à¹Œ / à¹à¸œà¸™à¸à¸—à¸µà¹ˆà¸¡à¸µà¹ƒà¸«à¹‰à¸šà¸£à¸´à¸à¸²à¸£ Details: à¸«à¸™à¹ˆà¸§à¸¢/à¸šà¸£à¸´à¸à¸²à¸£à¸—à¸µà¹ˆà¸¡à¸µà¹ƒà¸«à¹‰à¸šà¸£à¸´à¸à¸²à¸£: - à¹€à¸§à¸Šà¸£à¸°à¹€à¸šà¸µà¸¢à¸™ - à¸«à¹‰à¸­à¸‡ lab - à¸à¸²à¸£à¹€à¸‡à¸´à¸™ - à¸«à¹‰à¸­à¸‡à¸¢à¸² - OPD CMEx à¸Šà¸±à¹‰à¸™ 3 - CHC à¸Šà¸±à¹‰à¸™ 6\n",
      "ðŸŽ‰ RAG database update completed.\n",
      "\n",
      "---\n",
      "retrieval: dense | score: 0.04872465133666992 | row: 2\n",
      "Topic: à¹€à¸šà¸­à¸£à¹Œà¸•à¸´à¸”à¸•à¹ˆà¸­ / à¸Šà¹ˆà¸­à¸‡à¸—à¸²à¸‡à¸à¸²à¸£à¸™à¸±à¸” Details: à¹‚à¸—à¸£: 053-934742, 053-934710 LINE: @cmexcmu Facebook: à¸¨à¸¹à¸™à¸¢à¹Œà¸„à¸§à¸²à¸¡à¹€à¸›à¹‡à¸™à¹€à¸¥à¸´à¸¨à¸—à¸²à¸‡à¸à¸²à¸£à¹à¸žà¸—à¸¢à¹Œ Center for Medical Excellence\n",
      "\n",
      "---\n",
      "retrieval: dense | score: 0.04823070764541626 | row: 42\n",
      "Topic: à¸•à¸²à¸£à¸²à¸‡à¹à¸žà¸—à¸¢à¹Œà¸­à¸­à¸à¸•à¸£à¸§à¸ˆ: Allergy (à¹‚à¸£à¸„à¸ à¸¹à¸¡à¸´à¹à¸žà¹‰à¹à¸¥à¸°à¸ à¸¹à¸¡à¸´à¸„à¸¸à¹‰à¸¡à¸à¸±à¸™) Details: à¸•à¸²à¸£à¸²à¸‡à¹à¸žà¸—à¸¢à¹Œ (à¸•à¸²à¸¡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¹ƒà¸«à¹‰à¸¡à¸²): - à¸¨à¸¸à¸à¸£à¹Œ: à¸­.à¸žà¸.à¸à¸™à¸à¸à¸²à¸à¸ˆà¸™à¹Œ (09:00â€“12:00)\n",
      "\n",
      "---\n",
      "retrieval: dense | score: 0.041828811168670654 | row: 19\n",
      "Topic: à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­à¹à¸žà¸—à¸¢à¹Œà¸›à¸£à¸°à¸ˆà¸³: à¹‚à¸£à¸„à¸ à¸¹à¸¡à¸´à¹à¸žà¹‰ (Allergy/Immunology) Details: à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­à¹à¸žà¸—à¸¢à¹Œ: - à¸žà¸.à¸à¸™à¸à¸à¸²à¸à¸ˆà¸™à¹Œ à¸ à¸´à¸à¹‚à¸à¸žà¸£à¸žà¸²à¸“à¸´à¸Šà¸¢à¹Œ â€” Kanokkarn Pinyopornpanish, MD.\n",
      "\n",
      "---\n",
      "retrieval: sparse | score: 0.0 | row: 0\n",
      "Topic: à¸šà¸£à¸´à¸à¸²à¸£à¸‚à¸­à¸‡à¸¨à¸¹à¸™à¸¢à¹Œ / à¹à¸œà¸™à¸à¸—à¸µà¹ˆà¸¡à¸µà¹ƒà¸«à¹‰à¸šà¸£à¸´à¸à¸²à¸£ Details: à¸«à¸™à¹ˆà¸§à¸¢/à¸šà¸£à¸´à¸à¸²à¸£à¸—à¸µà¹ˆà¸¡à¸µà¹ƒà¸«à¹‰à¸šà¸£à¸´à¸à¸²à¸£: - à¹€à¸§à¸Šà¸£à¸°à¹€à¸šà¸µà¸¢à¸™ - à¸«à¹‰à¸­à¸‡ lab - à¸à¸²à¸£à¹€à¸‡à¸´à¸™ - à¸«à¹‰à¸­à¸‡à¸¢à¸² - OPD CMEx à¸Šà¸±à¹‰à¸™ 3 - CHC à¸Šà¸±à¹‰à¸™ 6\n",
      "\n",
      "---\n",
      "retrieval: sparse | score: 0.0 | row: 27\n",
      "Topic: à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­à¹à¸žà¸—à¸¢à¹Œà¸›à¸£à¸°à¸ˆà¸³: à¸¨à¸±à¸¥à¸¢à¸à¸£à¸£à¸¡à¸¨à¸µà¸£à¸©à¸° à¸„à¸­ à¹à¸¥à¸°à¹€à¸•à¹‰à¸²à¸™à¸¡ (Head/Neck/Breast) Details: à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­à¹à¸žà¸—à¸¢à¹Œ: - à¸žà¸.à¸­à¸²à¸£à¸µà¸§à¸£à¸£à¸“ à¸ªà¸¡à¸«à¸§à¸±à¸‡à¸›à¸£à¸°à¹€à¸ªà¸£à¸´à¸ â€” Areewan Somwangprasert, MD. - à¸™à¸ž.à¸ˆà¸±à¸à¸£à¸à¸£à¸´à¸Š à¸”à¸´à¸©à¸˜à¸£à¸£à¸¡ â€” Chagkrit Ditsatham, MD. - à¸žà¸.à¸ˆà¸¸à¸¬à¸²à¸£à¸±à¸•à¸™à¹Œ à¸”à¸§à¸‡à¹à¸à¹‰à¸§ â€” Chularat Duangkaew, MD.\n",
      "\n",
      "---\n",
      "retrieval: sparse | score: 0.0 | row: 28\n",
      "Topic: à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­à¹à¸žà¸—à¸¢à¹Œà¸›à¸£à¸°à¸ˆà¸³: à¸ˆà¸´à¸•à¹€à¸§à¸Š (Psychiatry) Details: à¸£à¸²à¸¢à¸Šà¸·à¹ˆà¸­à¹à¸žà¸—à¸¢à¹Œ: - à¸žà¸.à¸ªà¸¸à¸£à¸´à¸™à¸—à¸£à¹Œà¸žà¸£ à¸¥à¸´à¸‚à¸´à¸•à¹€à¸ªà¸–à¸µà¸¢à¸£ â€” Surinphon Likhitsathian, MD.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import math\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def chunk_text(text: str, max_length: int = 0, overlap: int = 0) -> List[str]:\n",
    "    \"\"\"Character-based chunking with optional overlap.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    if max_length <= 0:\n",
    "        return [text.strip()]\n",
    "    chunks = []\n",
    "    start, n = 0, len(text)\n",
    "    while start < n:\n",
    "        end = min(start + max_length, n)\n",
    "        piece = text[start:end].strip()\n",
    "        if piece:\n",
    "            chunks.append(piece)\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def _encode(documents: List[str], model_name: str) -> np.ndarray:\n",
    "    if not documents:\n",
    "        raise ValueError(\"No documents to encode.\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    vecs = model.encode(documents, show_progress_bar=True, normalize_embeddings=True)\n",
    "    return np.asarray(vecs, dtype=\"float32\")\n",
    "\n",
    "\n",
    "def _build_faiss(vectors: np.ndarray) -> faiss.Index:\n",
    "    dim = int(vectors.shape[1])\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(vectors)\n",
    "    return index\n",
    "\n",
    "\n",
    "def _build_tfidf(documents: List[str]) -> Tuple[TfidfVectorizer, sparse.csr_matrix]:\n",
    "    vec = TfidfVectorizer()\n",
    "    mat = vec.fit_transform(documents)\n",
    "    return vec, mat\n",
    "\n",
    "\n",
    "def _save_artifacts(\n",
    "    output_dir: str,\n",
    "    prefix: str,\n",
    "    info_dict: Dict[str, Any],\n",
    "    vectors: np.ndarray,\n",
    "    faiss_index: faiss.Index,\n",
    "    tfidf_vectorizer: TfidfVectorizer,\n",
    "    tfidf_matrix: sparse.csr_matrix,\n",
    "):\n",
    "    _ensure_dir(output_dir)\n",
    "\n",
    "    info_path = os.path.join(output_dir, f\"{prefix}_info.json\")\n",
    "    with open(info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(info_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    faiss_path = os.path.join(output_dir, f\"{prefix}_faiss_index.bin\")\n",
    "    faiss.write_index(faiss_index, faiss_path)\n",
    "\n",
    "    vec_path = os.path.join(output_dir, f\"{prefix}_tfidf_vectorizer.pkl\")\n",
    "    with open(vec_path, \"wb\") as f:\n",
    "        pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "    mat_path = os.path.join(output_dir, f\"{prefix}_tfidf_matrix.npz\")\n",
    "    sparse.save_npz(mat_path, tfidf_matrix)\n",
    "\n",
    "    print(f\"âœ… Saved: {info_path}\")\n",
    "    print(f\"âœ… Saved: {faiss_path}\")\n",
    "    print(f\"âœ… Saved: {vec_path}\")\n",
    "    print(f\"âœ… Saved: {mat_path}\")\n",
    "    print(f\"Embedding dim: {vectors.shape[1]} | #docs: {vectors.shape[0]}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Knowledge CSV â†’ 1 row = 1 chunk\n",
    "# - JSON keeps ALL metadata columns\n",
    "# - RAG text uses only topic_title + details\n",
    "# =========================\n",
    "REQUIRED_FOR_RAG = [\"topic_title\", \"details\"]\n",
    "\n",
    "\n",
    "def _compose_rag_text(topic_title: str, details: str) -> str:\n",
    "    topic = (topic_title or \"\").strip()\n",
    "    det = (details or \"\").strip()\n",
    "\n",
    "    if topic and det:\n",
    "        return f\"Topic: {topic}\\nDetails: {det}\"\n",
    "    if topic:\n",
    "        return f\"Topic: {topic}\"\n",
    "    if det:\n",
    "        return f\"Details: {det}\"\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def load_from_knowledge_csv(csv_path: str) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    - Reads CSV (UTF-8-sig).\n",
    "    - Each row becomes exactly 1 chunk (no splitting).\n",
    "    - Metadata: ALL columns preserved.\n",
    "    - Chunk text: ONLY topic_title + details.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "    missing = [c for c in REQUIRED_FOR_RAG if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"CSV missing required columns for RAG: {missing}. \"\n",
    "            f\"Your CSV must include at least: {REQUIRED_FOR_RAG}\"\n",
    "        )\n",
    "\n",
    "    info: Dict[str, Dict[str, Any]] = {}\n",
    "    idx = 0\n",
    "\n",
    "    for row_i in range(len(df)):\n",
    "        # Preserve ALL columns as metadata\n",
    "        rec: Dict[str, Any] = {}\n",
    "        for col in df.columns:\n",
    "            val = df.at[row_i, col]\n",
    "            rec[col] = \"\" if pd.isna(val) else str(val)\n",
    "\n",
    "        # Build RAG chunk from ONLY topic_title + details\n",
    "        chunk = _compose_rag_text(\n",
    "            topic_title=rec.get(\"topic_title\", \"\"),\n",
    "            details=rec.get(\"details\", \"\"),\n",
    "        )\n",
    "        if not chunk.strip():\n",
    "            continue\n",
    "\n",
    "        rec[\"chunk\"] = chunk\n",
    "        rec[\"row_index\"] = int(row_i)\n",
    "        rec[\"chunk_idx\"] = 0  # one chunk per row\n",
    "\n",
    "        info[str(idx)] = rec\n",
    "        idx += 1\n",
    "\n",
    "    print(f\"Prepared {len(info)} knowledge chunks from {len(df)} rows (1 row = 1 chunk).\")\n",
    "    return info\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Orchestrator (knowledge_csv only here)\n",
    "# =========================\n",
    "def update_rag_database_from_knowledge_csv(\n",
    "    knowledge_csv: str,\n",
    "    output_dir: str,\n",
    "    prefix: str,\n",
    "    embed_model: str,\n",
    "):\n",
    "    info = load_from_knowledge_csv(knowledge_csv)\n",
    "\n",
    "    if not info:\n",
    "        print(\"âš ï¸ No documents prepared. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Keep stable order: 0..N-1\n",
    "    documents = [info[k][\"chunk\"] for k in sorted(info.keys(), key=lambda x: int(x))]\n",
    "\n",
    "    vectors = _encode(documents, model_name=embed_model)\n",
    "    index = _build_faiss(vectors)\n",
    "    tfidf_vec, tfidf_mat = _build_tfidf(documents)\n",
    "\n",
    "    _save_artifacts(\n",
    "        output_dir=output_dir,\n",
    "        prefix=prefix,\n",
    "        info_dict=info,\n",
    "        vectors=vectors,\n",
    "        faiss_index=index,\n",
    "        tfidf_vectorizer=tfidf_vec,\n",
    "        tfidf_matrix=tfidf_mat,\n",
    "    )\n",
    "\n",
    "    print(\"First chunk:\", documents[0][:200].replace(\"\\n\", \" \"))\n",
    "    print(\"ðŸŽ‰ RAG database update completed.\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Query / Retrieve (Hybrid)\n",
    "# =========================\n",
    "def load_rag_artifacts(output_dir: str, prefix: str):\n",
    "    info_path = os.path.join(output_dir, f\"{prefix}_info.json\")\n",
    "    faiss_path = os.path.join(output_dir, f\"{prefix}_faiss_index.bin\")\n",
    "    vec_path = os.path.join(output_dir, f\"{prefix}_tfidf_vectorizer.pkl\")\n",
    "    mat_path = os.path.join(output_dir, f\"{prefix}_tfidf_matrix.npz\")\n",
    "\n",
    "    with open(info_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        info: Dict[str, Dict[str, Any]] = json.load(f)\n",
    "\n",
    "    index = faiss.read_index(faiss_path)\n",
    "\n",
    "    with open(vec_path, \"rb\") as f:\n",
    "        tfidf_vectorizer: TfidfVectorizer = pickle.load(f)\n",
    "\n",
    "    tfidf_matrix = sparse.load_npz(mat_path)\n",
    "\n",
    "    # stable order: 0..N-1\n",
    "    keys = sorted(info.keys(), key=lambda x: int(x))\n",
    "    return info, keys, index, tfidf_vectorizer, tfidf_matrix\n",
    "\n",
    "\n",
    "def _dense_scores_faiss(\n",
    "    query: str,\n",
    "    model: SentenceTransformer,\n",
    "    index: faiss.Index,\n",
    "    top_k_dense: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      dense_idx: shape (m,) indices into your doc list\n",
    "      dense_sim: shape (m,) cosine-like similarity in ~[-1,1]\n",
    "    \"\"\"\n",
    "    q = model.encode([query], normalize_embeddings=True)\n",
    "    q = np.asarray(q, dtype=\"float32\")\n",
    "\n",
    "    D, I = index.search(q, top_k_dense)  # shapes (1, k)\n",
    "    D = D[0]\n",
    "    I = I[0]\n",
    "\n",
    "    # If vectors are unit-normalized, squared L2 = 2 - 2*cos => cos = 1 - D/2\n",
    "    dense_sim = 1.0 - (D / 2.0)\n",
    "    dense_sim = np.clip(dense_sim, -1.0, 1.0)\n",
    "    return I, dense_sim\n",
    "\n",
    "\n",
    "def _sparse_scores_tfidf(\n",
    "    query: str,\n",
    "    tfidf_vectorizer: TfidfVectorizer,\n",
    "    tfidf_matrix: sparse.csr_matrix,\n",
    "    top_k_sparse: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    TF-IDF uses L2 norm by default -> dot product ~ cosine similarity.\n",
    "    Returns top indices and scores.\n",
    "    \"\"\"\n",
    "    qv = tfidf_vectorizer.transform([query])          # (1, vocab)\n",
    "    scores = (qv @ tfidf_matrix.T).toarray().ravel()  # (N,)\n",
    "\n",
    "    if top_k_sparse >= len(scores):\n",
    "        idx = np.argsort(-scores)\n",
    "    else:\n",
    "        idx = np.argpartition(-scores, top_k_sparse)[:top_k_sparse]\n",
    "        idx = idx[np.argsort(-scores[idx])]\n",
    "\n",
    "    return idx.astype(int), scores[idx].astype(float)\n",
    "\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str,\n",
    "    output_dir: str,\n",
    "    prefix: str,\n",
    "    embed_model_name: str,\n",
    "    top_k: int = 5,\n",
    "    top_k_dense: int = 50,\n",
    "    top_k_sparse: int = 200,\n",
    "    alpha: float = 0.6,\n",
    "    method: str = \"equal\",  # \"equal\" | \"weighted\" | \"rrf\"\n",
    "    rrf_k: int = 60,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Hybrid retrieval from saved artifacts.\n",
    "\n",
    "    method:\n",
    "      - \"equal\": take top ceil(K/2) dense + top floor(K/2) sparse (dedupe)\n",
    "      - \"weighted\": min-max normalize dense/sparse scores over union and combine with alpha\n",
    "      - \"rrf\": reciprocal rank fusion on dense/sparse rankings\n",
    "\n",
    "    Returns: list of dicts (metadata + chunk + score + retrieval)\n",
    "    \"\"\"\n",
    "    info, keys, index, tfidf_vec, tfidf_mat = load_rag_artifacts(output_dir, prefix)\n",
    "    model = SentenceTransformer(embed_model_name)\n",
    "\n",
    "    # Get candidates (always fetch enough for fusion + equal split)\n",
    "    k_dense_fetch = max(top_k_dense, top_k)\n",
    "    k_sparse_fetch = max(top_k_sparse, top_k)\n",
    "\n",
    "    dense_idx, dense_sim = _dense_scores_faiss(query, model, index, k_dense_fetch)\n",
    "    sparse_idx, sparse_sim = _sparse_scores_tfidf(query, tfidf_vec, tfidf_mat, k_sparse_fetch)\n",
    "\n",
    "    dense_idx_list = [int(i) for i in dense_idx.tolist()]\n",
    "    dense_sim_list = [float(s) for s in dense_sim.tolist()]\n",
    "    sparse_idx_list = [int(i) for i in sparse_idx.tolist()]\n",
    "    sparse_sim_list = [float(s) for s in sparse_sim.tolist()]\n",
    "\n",
    "    dense_map = {i: s for i, s in zip(dense_idx_list, dense_sim_list)}\n",
    "    sparse_map = {i: s for i, s in zip(sparse_idx_list, sparse_sim_list)}\n",
    "\n",
    "    # -------------------------\n",
    "    # METHOD: EQUAL SPLIT\n",
    "    # -------------------------\n",
    "    if method == \"equal\":\n",
    "        k_dense = math.ceil(top_k / 2)\n",
    "        k_sparse = top_k // 2\n",
    "\n",
    "        seen = set()\n",
    "        results: List[Dict[str, Any]] = []\n",
    "\n",
    "        # 1) dense first\n",
    "        for i in dense_idx_list[:k_dense]:\n",
    "            if i in seen:\n",
    "                continue\n",
    "            seen.add(i)\n",
    "            rec = dict(info[keys[i]])\n",
    "            rec[\"retrieval\"] = \"dense\"\n",
    "            rec[\"score\"] = float(dense_map.get(i, 0.0))\n",
    "            results.append(rec)\n",
    "            if len(results) >= top_k:\n",
    "                return results[:top_k]\n",
    "\n",
    "        # 2) sparse\n",
    "        for i in sparse_idx_list[:k_sparse]:\n",
    "            if i in seen:\n",
    "                continue\n",
    "            seen.add(i)\n",
    "            rec = dict(info[keys[i]])\n",
    "            rec[\"retrieval\"] = \"sparse\"\n",
    "            rec[\"score\"] = float(sparse_map.get(i, 0.0))\n",
    "            results.append(rec)\n",
    "            if len(results) >= top_k:\n",
    "                return results[:top_k]\n",
    "\n",
    "        # If dedupe reduced count, top up from remaining candidates\n",
    "        for i in dense_idx_list[k_dense:]:\n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "            if i in seen:\n",
    "                continue\n",
    "            seen.add(i)\n",
    "            rec = dict(info[keys[i]])\n",
    "            rec[\"retrieval\"] = \"dense\"\n",
    "            rec[\"score\"] = float(dense_map.get(i, 0.0))\n",
    "            results.append(rec)\n",
    "\n",
    "        for i in sparse_idx_list[k_sparse:]:\n",
    "            if len(results) >= top_k:\n",
    "                break\n",
    "            if i in seen:\n",
    "                continue\n",
    "            seen.add(i)\n",
    "            rec = dict(info[keys[i]])\n",
    "            rec[\"retrieval\"] = \"sparse\"\n",
    "            rec[\"score\"] = float(sparse_map.get(i, 0.0))\n",
    "            results.append(rec)\n",
    "\n",
    "        return results[:top_k]\n",
    "\n",
    "    # -------------------------\n",
    "    # METHOD: RRF\n",
    "    # -------------------------\n",
    "    if method == \"rrf\":\n",
    "        dense_rank = {i: r for r, i in enumerate(dense_idx_list, start=1)}\n",
    "        sparse_rank = {i: r for r, i in enumerate(sparse_idx_list, start=1)}\n",
    "\n",
    "        fused: List[Tuple[int, float]] = []\n",
    "        for i in set(dense_rank.keys()) | set(sparse_rank.keys()):\n",
    "            score = 0.0\n",
    "            rd = dense_rank.get(i)\n",
    "            rs = sparse_rank.get(i)\n",
    "            if rd is not None:\n",
    "                score += 1.0 / (rrf_k + rd)\n",
    "            if rs is not None:\n",
    "                score += 1.0 / (rrf_k + rs)\n",
    "            fused.append((i, score))\n",
    "\n",
    "        fused.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for i, score in fused[:top_k]:\n",
    "            rec = dict(info[keys[i]])\n",
    "            rec[\"retrieval\"] = \"rrf\"\n",
    "            rec[\"score\"] = float(score)\n",
    "            results.append(rec)\n",
    "        return results\n",
    "\n",
    "    # -------------------------\n",
    "    # METHOD: WEIGHTED\n",
    "    # -------------------------\n",
    "    if method == \"weighted\":\n",
    "        cand = sorted(set(dense_map.keys()) | set(sparse_map.keys()))\n",
    "        d_scores = np.array([dense_map.get(i, 0.0) for i in cand], dtype=float)\n",
    "        s_scores = np.array([sparse_map.get(i, 0.0) for i in cand], dtype=float)\n",
    "\n",
    "        def minmax(x: np.ndarray) -> np.ndarray:\n",
    "            if x.size == 0:\n",
    "                return x\n",
    "            lo, hi = float(x.min()), float(x.max())\n",
    "            if hi - lo < 1e-12:\n",
    "                return np.zeros_like(x)\n",
    "            return (x - lo) / (hi - lo)\n",
    "\n",
    "        d_norm = minmax(d_scores)\n",
    "        s_norm = minmax(s_scores)\n",
    "\n",
    "        fused_scores = alpha * d_norm + (1.0 - alpha) * s_norm\n",
    "        fused = sorted(zip(cand, fused_scores.tolist()), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        results: List[Dict[str, Any]] = []\n",
    "        for i, score in fused[:top_k]:\n",
    "            rec = dict(info[keys[i]])\n",
    "            rec[\"retrieval\"] = \"weighted\"\n",
    "            rec[\"score\"] = float(score)\n",
    "            results.append(rec)\n",
    "        return results\n",
    "\n",
    "    raise ValueError('method must be one of: \"equal\", \"weighted\", \"rrf\"')\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Example\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Build/update DB\n",
    "    update_rag_database_from_knowledge_csv(\n",
    "        knowledge_csv=\"./chunks.csv\",\n",
    "        output_dir=\"./RAG_database\",\n",
    "        prefix=\"knowledge\",\n",
    "        embed_model=\"all-MiniLM-L6-v2\",\n",
    "    )\n",
    "\n",
    "    # 2) Query\n",
    "    results = hybrid_search(\n",
    "        query=\"reset password email not received\",\n",
    "        output_dir=\"./RAG_database\",\n",
    "        prefix=\"knowledge\",\n",
    "        embed_model_name=\"all-MiniLM-L6-v2\",\n",
    "        top_k=6,\n",
    "        method=\"equal\",  # <- your requested default method\n",
    "    )\n",
    "\n",
    "    for r in results:\n",
    "        print(\"\\n---\")\n",
    "        print(\"retrieval:\", r[\"retrieval\"], \"| score:\", r[\"score\"], \"| row:\", r.get(\"row_index\"))\n",
    "        print(r[\"chunk\"][:250].replace(\"\\n\", \" \"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "askicdi_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
